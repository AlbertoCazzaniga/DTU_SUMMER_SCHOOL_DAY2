{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoCazzaniga/DTU_SUMMER_SCHOOL_DAY2/blob/main/DTU_SUMMER_SCHOOL_DAY2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DTU SUMMER SCHOOL: ADVANCED MACHINE LEARNING\n",
        "## LAB SESSION ON INTRINSIC DIMENSION AND NEIGHBORHOOD OVERLAP ANALYSIS FOR REPRESENTATIONS FROM SELF-SUPERVISION\n",
        "\n",
        "### Copenaghen, Tuesday 22/08/2023\n",
        "\n",
        "\n",
        "\n",
        "Alberto Cazzaniga: alberto.cazzaniga@areasciencepark.it\n",
        "\n",
        "Alessio Ansuini: alessio.ansuini@areasciencepark.it"
      ],
      "metadata": {
        "id": "746DsUFBdRB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "T1O4l7w51816"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PROBLEM 1: PROTEIN LANGUAGE MODELS REPRESENTATIONS**\n",
        "\n",
        "\n",
        "Dropbox folder with data available: https://www.dropbox.com/scl/fo/tav4bk5smn1zxf4zyoetn/h?rlkey=yc0d7iubctfkc0p82ddukwr3g&dl=0"
      ],
      "metadata": {
        "id": "EVF4ACikOC42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given two ESM models, we will focus on understanding how the neighboorhod structure is changing between their corresponding embeddings by means of the Neighborhood Overlap method.\n",
        "\n",
        "In particular, we want to understand if the neighboors composition is conserved or not in proximity of the *plateau* and the *peak* phase of the ID curve Figure 1.\n"
      ],
      "metadata": {
        "id": "6NfcswSlM-cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&\n",
        "id=1SctShFfLXNUDGPZJJQ8HKBa_6uyM7f5j' width=\"400\" />\n",
        "<figcaption> Figure 1: ID curve of ESM2 models. </figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "e9OZVS19SRw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More formally:\n",
        "\n",
        "- consider two models $X, Y \\in$ {ESM2-35M, ESM2-650M, ESM2-3B};\n",
        "\n",
        "- for a given dataset we compare the embeddings at layer $l$ of the model $X$ and at layer $m$ of the model $Y$;\n",
        "\n",
        "- fix an integer $k$ that represent the \"size\" of the neighborhood we consider;\n",
        "\n",
        "- compute neighborhood overlap between layer $l$ (of $X$) and $m$ (of $Y$) through the formula:\n",
        "\n",
        "$$\\chi_k^{l,m} = \\frac{1}{N} \\sum_i \\frac{1}{k} \\sum_j {A^l}_{ij} A^m_{ij}$$\n",
        "\n",
        "\n",
        "We will perform this operation for all pairs of layers of $X$ and $Y$, thus obtaining $depth(X) \\times depth(Y)$ values that we will plot as an heatmap."
      ],
      "metadata": {
        "id": "qun0nL4uOK0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements:\n",
        "\n",
        "*  `matplotlib`\n",
        "*   `seaborn`\n",
        "*   `numpy`\n",
        "*  [DADApy](https://dadapy.readthedocs.io/en/latest/)\n",
        "*  [ESM](https://github.com/facebookresearch/esm)\n",
        "\n"
      ],
      "metadata": {
        "id": "6yLQhfIGOOBf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kgvYvSLN7lH"
      },
      "outputs": [],
      "source": [
        "#!pip install seaborn\n",
        "#!pip install numpy\n",
        "#!pip install matplotlib\n",
        "!pip install git+https://github.com/sissa-data-science/DADApy\n",
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dadapy import Data\n",
        "from dadapy import *\n",
        "\n",
        "import torch\n",
        "import esm\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 10]"
      ],
      "metadata": {
        "id": "gDWOFvXkOf0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose the models you want to compare"
      ],
      "metadata": {
        "id": "6SbzqQX2USTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {'35M':['ESM2-35M', 12],\n",
        "          '650M':['ESM2-650M', 33],\n",
        "          '3B':['ESM2-3B', 36]}"
      ],
      "metadata": {
        "id": "4ZqqK2_qOgpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_model = '650M'\n",
        "y_model = '35M'\n",
        "\n",
        "reps_x = np.arange(models[x_model][1]+1)\n",
        "reps_y = np.arange(models[y_model][1]+1)"
      ],
      "metadata": {
        "id": "ZV2c2i7ePp3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to perform inference with ESM model\n",
        "(from https://github.com/facebookresearch/esm)"
      ],
      "metadata": {
        "id": "bZ3tDDUXDg3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "BYo-aA44Dl7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the data\n",
        "data = [(\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
        "        (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
        "        (\"protein3\", \"KALTARQQEVFDLIRDISQTKGVIEIVSGASRGIRLLQEE\")]\n",
        "\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "# Choose which representations you want to extract\n",
        "reps = [r for r in range(34)]\n",
        "\n",
        "# Extract per-residue representations\n",
        "with torch.no_grad():\n",
        "    results = model(batch_tokens, repr_layers = reps)\n",
        "\n",
        "# Extract last layer representation only\n",
        "token_representations = results[\"representations\"][33]\n",
        "\n",
        "# Generate per-sequence representations via averaging\n",
        "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "sequence_representations = []\n",
        "for i, tokens_len in enumerate(batch_lens):\n",
        "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))"
      ],
      "metadata": {
        "id": "HyNvLCjVECB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the neighbor indexes and compute neighboorhood overlap\n",
        "\n",
        "For 5000 proteins (nsamples) chosen in the ProteinNet dataset, we pre-computed the embeddings of the ESM models (for time saving).\n",
        "\n",
        "You can download the representations from all the layer of the $x\\_model$ and $y\\_model$ running the following block of code.\n",
        "\n",
        "Of course you are welcome to independently try to mimic this."
      ],
      "metadata": {
        "id": "aOtjOkr1UYox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if x_model == '650M' or y_model == '650M':\n",
        "  !wget https://www.dropbox.com/scl/fi/feynf2trfzal4lvhrkcfm/ESM2-650M.tar.gz?rlkey=zfkcm7xebgfn1zymehll6tz52&dl=1\n",
        "  !tar -zxvf ESM2-650M.tar.gz?rlkey=zfkcm7xebgfn1zymehll6tz52&dl=1\n",
        "\n",
        "elif x_model == '3B' or y_model == '3B':\n",
        "  !wget https://www.dropbox.com/scl/fi/m6it8rlblfpek15zzqle8/ESM2-3B.tar.gz?rlkey=g6f9zv5tdzil2xt6j4ni7zwg8&dl=1\n",
        "  !tar -zxvf ESM2-3B.tar.gz?rlkey=g6f9zv5tdzil2xt6j4ni7zwg8&dl=1\n",
        "\n",
        "if  x_model == '35M' or y_model == '35M':\n",
        "  !wget https://www.dropbox.com/scl/fi/bz7dipe6ma38pc5oic92x/ESM2-35M.tar.gz?rlkey=663j56iaq4v9yr98r03spboce&dl=1\n",
        "  !tar -zxvf ESM2-35M.tar.gz?rlkey=663j56iaq4v9yr98r03spboce&dl=1"
      ],
      "metadata": {
        "id": "4ZtvHWgTYWpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neighboor overlap is computed using the indexes of the k-nearest neighoboor (k-NN) of protein in our dataset (where max_k is 50).\n",
        "\n",
        "For time constraints these have been already computed, and can be downloaded running the cell below.\n",
        "\n",
        "As before, you are more then welcome to try your favourite Nearest Neighbor estimator to compute k-NN indices for the representations."
      ],
      "metadata": {
        "id": "ezLjGW9AUWRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if x_model == '650M' or y_model == '650M':\n",
        "  !wget https://www.dropbox.com/scl/fi/z8rcq18lih79lo585g7iv/ESM2-650M-idx.tar.gz?rlkey=ydgt8urwon2g3mffue4g4wyaj&dl=1\n",
        "  !tar -zxvf ESM2-650M-idx.tar.gz?rlkey=ydgt8urwon2g3mffue4g4wyaj&dl=1\n",
        "\n",
        "elif x_model == '3B' or y_model == '3B':\n",
        "  !wget https://www.dropbox.com/scl/fi/rgiy194u15y4dtzd56dby/ESM2-3B-idx.tar.gz?rlkey=l2aq5nupr666s6dsu0mi52rbo&dl=1\n",
        "  !tar -zxvf ESM2-3B-idx.tar.gz?rlkey=l2aq5nupr666s6dsu0mi52rbo&dl=1\n",
        "\n",
        "if  x_model == '35M' or y_model == '35M':\n",
        "  !wget https://www.dropbox.com/scl/fi/1wpl2odsg0kyjvwg4y1b9/ESM2-35M-idx.tar.gz?rlkey=qanvf3sp2fozfvqo1zaxcx06o&dl=1\n",
        "  !tar -zxvf ESM2-35M-idx.tar.gz?rlkey=qanvf3sp2fozfvqo1zaxcx06o&dl=1"
      ],
      "metadata": {
        "id": "lAK8GmizUqxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have downloades the indexes for each proteins you can write the NO function that takes as input 2 of these arrays."
      ],
      "metadata": {
        "id": "BvkAwF1pUrss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write the function to calculate the neighboor overlap between two embeddings:"
      ],
      "metadata": {
        "id": "RlFSpc26qLb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we provide only the first k = 50 nearest neighbors for each point, one has to set k less or equal than 50 for the function to work."
      ],
      "metadata": {
        "id": "FYP4Hb-uVKla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neig_overlap(X, Y, K = 10):\n",
        "#     \"\"\"Computes the neighborhood overlap between two representations.\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     indices1 : 2D array of ints\n",
        "#         nearest neighbor index matrix of the first representation\n",
        "#     indices2 : 2D array of ints\n",
        "#         nearest neighbor index matrix of the second representation\n",
        "#     k : int\n",
        "#         number of nearest neighbors used to compute the overlap\n",
        "\n",
        "#     Returns\n",
        "#     -------\n",
        "#     overlap : float\n",
        "#         neighborhood overlap between the two representations\n",
        "#     \"\"\"\n",
        "#     assert indices1.shape[0] == indices2.shape[0]\n",
        "#     ndata = indices1.shape[0]\n",
        "#     overlaps = np.empty(ndata)\n",
        "\n",
        "#     for i in range(ndata):\n",
        "#         overlaps[i] = ...\n",
        "#     return np.mean(overlaps)"
      ],
      "metadata": {
        "id": "rgIm5zVNqTt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop over all the reps and obtain the final result:\n",
        "\n"
      ],
      "metadata": {
        "id": "ghQ2-6_5qZbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run instead of your function the DADapy method that requires as input the 2 embeddings."
      ],
      "metadata": {
        "id": "Pvgw3rS1ZERo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary where to store the results\n",
        "overlaps = {k: [] for k in reps_x}\n",
        "\n",
        "# Define the number of neighboor you want to consider\n",
        "k = 10"
      ],
      "metadata": {
        "id": "QcxNxOa-K7Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over all layer of model X\n",
        "for x in reps_x:\n",
        "\n",
        "        # Load x neig indexes\n",
        "        idx_x = np.load(f'{models[x_model][0]}-idx/{models[x_model][0]}_rep{x}_5000-idx.npy')\n",
        "\n",
        "        # Loop over all layers of model Y\n",
        "        for y in reps_y:\n",
        "\n",
        "            # Load y neig indexes\n",
        "            idx_y = np.load(f'{models[y_model][0]}-idx/{models[y_model][0]}_rep{y}_5000.npy')\n",
        "\n",
        "            # You call here your function\n",
        "            ov = neig_overlap(X = idx_x, Y = idx_y, K = k)\n",
        "\n",
        "            overlaps[x].append(ov)\n"
      ],
      "metadata": {
        "id": "8MFaeTHqU4Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect everything together\n",
        "all_overlaps = []\n",
        "for rep in reps_x:\n",
        "    all_overlaps.append(overlaps[rep])\n",
        "all_overlaps = np.stack(all_overlaps)"
      ],
      "metadata": {
        "id": "qd6qxnyNVpMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(data = all_overlaps,\n",
        "                annot = False,\n",
        "                cmap = \"viridis\")\n",
        "\n",
        "ax.set(xlabel = x_model,\n",
        "       ylabel = y_model)\n",
        "\n",
        "ax.set_title(r'$\\chi^{l,m}$')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yt-9wVpcTQG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the neighborhood overlaps already computed for ESM2-3B and ESM2-650M."
      ],
      "metadata": {
        "id": "8XZQJXB0qgD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download already computed overlaps and plot them\n",
        "!wget https://www.dropbox.com/scl/fi/gno1iigb26u0nwkkjwy6z/overlap.tar.gz?rlkey=f1nun3m0yul72604wo6feilx4&dl=1\n",
        "!tar -zxvf overlap.tar.gz?rlkey=f1nun3m0yul72604wo6feilx4&dl=1"
      ],
      "metadata": {
        "id": "kw4U8uV0lhzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the results."
      ],
      "metadata": {
        "id": "CyHXaeK7vQ1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = './overlap/3B_650M_10_5000.npy'\n",
        "xlab = file.split('/')[-1].split('_')[1]\n",
        "ylab = file.split('/')[-1].split('_')[0]\n",
        "\n",
        "ovs = np.load(file)\n",
        "ax = sns.heatmap(data = ovs[1:,1:],\n",
        "                annot = False,\n",
        "                cmap = \"viridis\")\n",
        "\n",
        "ax.set(xlabel = xlab,\n",
        "       ylabel = ylab)\n",
        "\n",
        "ax.set_title(r'$\\chi^{l,m}$')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ab8UvWNtChN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All we did so far was aimed to get to understand the basics in order to tackle this final analysis.\n",
        "\n",
        "Using neighborhood overlap, discuss similarities and differences between representations of the ESM2 models (8M,32M,150M,650M,3B) focusing on:\n",
        "\n",
        "- influence of the relative scale (i.e. relative number of parameters) of the models;\n",
        "- influence of the position of layers within the network (e.g. layer number);\n",
        "- relations with the intrinsic dimension graph in Figure 1."
      ],
      "metadata": {
        "id": "5hodyY-MvVak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "eEI1WMh81z_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **iGPT lab session**\n",
        "\n"
      ],
      "metadata": {
        "id": "tP3gOPleoWFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comparision intrinsic dimension profiles between iGPT and iBERT**\n",
        "\n"
      ],
      "metadata": {
        "id": "zey2xtJgtqMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&\n",
        "id=16dwmsUi5LFSf_Vc7LHfFMZ3RdIpmfyKs' width=\"400\" />\n",
        "<figcaption> Figure 2: ID curve of iGPT-large. </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "Figure 2 shows the intrinsic dimension (ID) profile of the ImageNet dataset in iGPT large. The scope of this section is to plot the ID on a subset of the iBERT representations.\n",
        "\n",
        "### **Overview: the two nearest neighbor estimator (TwoNN)**\n",
        "\n",
        "The TwoNN estimator measures the ID of a dataset with the ratios between the distances of each point to its second and first nearest neighbor.\n",
        "\n",
        "$$\\mu_i = \\dfrac{r_{i_2}}{r_{i_1} }$$\n",
        "\n",
        "The likelihood distribtuion of the $μ_i$ follows a Parteo law parametrized by the intrinsic dimension $d$ of the data.\n",
        "\n",
        "\n",
        "$$p(\\mu_i|d) = \\dfrac{d}{\\mu_i^{d+1}}$$\n",
        "\n",
        "<!-- <figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&\n",
        "id=10x2A-oixnyc2gx_kihxh-uKOIM-EcEsP' width=\"400\" />\n",
        "<figcaption> Figure 2: ID curve of iGPT-large. </figcaption></center>\n",
        "</figure> -->\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Given the $μ_i$ from the $N$ datapoints the intrinsic dimension $d$ can be estimated by maximum likelihood with the formula:\n",
        "\n",
        "\n",
        "$$    \\hat{d} = \\dfrac{N}{\\sum_{i=1}^N \\text{log} \\mu_i} $$\n",
        "\n",
        "\n",
        "\n",
        "### **Download data**\n",
        "Download the distances from each data point to its first and second neighbor:"
      ],
      "metadata": {
        "id": "h2zPWcQtK70R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!gdown --id 1JFiKWYOl9TmPISJ872n1X3TarfWR-Tzl\n",
        "!unzip -o igpt_data.zip\n",
        "!rm igpt_data.zip"
      ],
      "metadata": {
        "id": "sSpIMJKOSxVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect folder structure"
      ],
      "metadata": {
        "id": "BQuKmtucjj4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tree &> /dev/null\n",
        "!tree -d igpt_data"
      ],
      "metadata": {
        "id": "xzKBLxbVjp8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load distance matrices ibert representations:\n",
        "\n",
        "The distances from each data representations to its first two nearest neighbors are stored in **igpt_data/ibert/post_mlp**.\n",
        "\n",
        "In this folder, you find the nearest neighbor distances of the the outputs of the multi-layer perceptron after layer normalization.\n",
        "\n",
        "You find the distance matrix of the layers $\\{1, 3, 5, 7, ... 49 \\}$. You can load the distance matrix of, e.g., the layer \"1\" with:\n",
        "\n",
        "```\n",
        "folder = \"igpt_data/ibert/post_mlp\"\n",
        "distances = np.load(folder +\"/layer1_post_mlp_dist.npy\")\n",
        "```\n",
        "The first column of the distance matrix, distances[:, 0], is full of zeros as it contains the distances form a datapoint to itself. distances[:, 1], distances[:, 2] contain the distances between each data points and its first and second neighbor respectively.\n",
        "```\n",
        "print(distances)\n",
        "\n",
        "[[0, 0.12, 0.17]\n",
        "[0, 0.12, 0.17]\n",
        "...\n",
        "[0, 0.16, 0.16]]\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VvIMQ59PiBlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compute the intrinsic dimension of the iBERT representations.**\n",
        "Implement the TwoNN intrinsic dimension estimate of the iBERT representations with the formula provided above:\n",
        "\n",
        "#### **Important note**\n",
        "\n",
        "When computing the ID an important euristic is used. Facco et al. recommend removing the 10% of $\\mu_i$ with the largest values.\n",
        "\n",
        "You can do this as follows:\n",
        "\n",
        "```\n",
        "# N ->  number of datapoints\n",
        "# mu_fraction = 0.9 -> number of mus to keep in the estimate\n",
        "N_eff = int(N * mu_fraction)\n",
        "mus_reduced = np.sort(mus)[:N_eff]\n",
        "```\n",
        "Then use **N_eff** and **mus_reduced** in the maximum likelihood esimate of the ID provided above.\n",
        "\n",
        "\n",
        "**1. Implement the TwoNN estimator**\n",
        "- Compute the $μ_i$ ratios from the distance matrix.\n",
        "- Use the $μ_i$ to compute the ID.\n",
        "\n",
        "**2. Compare how N_eff affects the ID estimate.**\n",
        "- How does the ID profile change if we use all the $μ_i$?\n",
        "\n",
        "**3. Plot the ID profiles of iBERT and compare to those of iGPT shown above.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YAz1-lE9S-7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement the TwoNN estimator below\n"
      ],
      "metadata": {
        "id": "0njQnBrlcJgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_id_2nn(distances, mu_fraction = 0.9):+\n",
        "#     \"\"\"TwoNN intrinsic dimension estimator.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     distances : 2D array of floats\n",
        "#         nearest neighbor distance matrix\n",
        "#     mu_fraction : float\n",
        "#         fraction of mus to consider in the ID estimate (see explanation above)\n",
        "\n",
        "#     Returns\n",
        "#     -------\n",
        "#     intrinsic_dim : float\n",
        "#         intrinsic dimension estimated with TwoNN\n",
        "#     \"\"\"\n",
        "#     mus =                       #mu_ratios\n",
        "#     N =                         #number of datapoints\n",
        "#     N_eff =                     #fraction of datapoints used to compute the ID\n",
        "#     mus_reduced =               #mus kept to estimated ID\n",
        "#     intrisic_dim =              #id estimate\n",
        "#     return intrinsic_dim"
      ],
      "metadata": {
        "id": "fbhuEZ6pTN4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compute the ID on the iBERT representations; try to see how the euristic on the $\\mu$'s affect the ID estimate."
      ],
      "metadata": {
        "id": "jq92qd8RWJVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#layers = np.arange(1, 50, 2)\n",
        "#intrinsic_dimensions = []\n",
        "#for layer in layers:\n",
        "#  distances = np.load(f\"igpt_data/ibert/post_mlp/layer{layer}_post_mlp_dist.npy\")\n",
        "#   intrinsic_dimensions.append(compute_id_2nn(distances, mu_fraction = ...))\n"
      ],
      "metadata": {
        "id": "mEXbZCdRazTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Plot the ID of the bert representations and compare with those of iGPT shown above"
      ],
      "metadata": {
        "id": "SBeIHv0T-741"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## write here the code of your plot"
      ],
      "metadata": {
        "id": "WzBKyLLO_CVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optional: Exploring intrinsic dimension profiles in other layers of iGPT**\n",
        "\n",
        "In *Valeriani et al.* the various statistics for iGPT are measured **after the first normalization layer** of each attention block. In this section, we want to explore how the profile changes if we measure the ID and the overlap in other positions of the attention blocks: after the normalization layer next to the self attention block, and before the first layer normalization.\n",
        "\n",
        "The structure of the attention blocks in iGPT and iBERT is as follows:\n",
        "\n",
        "1. Layer Noramlization\n",
        "2. Self Attention\n",
        "3. Layer Normalization\n",
        "4. Multi-layer Perceptron\n",
        "\n",
        "\n",
        "Compare the intrinsic dimenstion profiles in iGPT with the represerntation estracted:\n",
        "1. **before** the first normalization layer;\n",
        "2. **after** the first normalization layer (this is what you alredy computed above);\n",
        "3. **after** the second normalization layer.\n",
        "\n",
        "You can load the nearest neighbor distance matrices for these representations as:\n",
        "\n",
        "```\n",
        "#before the first normalization layer\n",
        "distances = np.load(f\"igpt_data/igpt/pre_ln/layer{lay}_post_mlp_dist.npy\")\n",
        "\n",
        "#after the first normalization layer\n",
        "distances = np.load(f\"igpt_data/igpt/post_mlp/layer{lay}_post_mlp_dist.npy\")\n",
        "\n",
        "#after the second normalization layer\n",
        "distances = np.load(f\"igpt_data/igpt/post_attn/layer{lay}_post_mlp_dist.npy\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "sFrnt_oQ_4FR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Similarity of iBERT and iGPT representations**\n",
        "\n",
        "In this section we will compare the representations of the ImageNet dataset at identical layers of the iGPT and iBERT networks. We will use $\\chi_k^{l_{GPT}, l_{BERT} }$, the neighborhood overlap between pairs of representations. $\\chi_k^{l_{GPT}, l_{BERT} }$ measures the fraction of common neighbors within the first $k$ averaged over the dataset.\n",
        "\n",
        "\n",
        "Given the two adjacency lists of lenght $k$ in two represenatations $l_{BERT}$, $l_{GPT}$, the overlap *for a datapoint $i$* is computed as:\n",
        "\n",
        "\n",
        "$$\\chi_{k_i}^{l_{GPT}, l_{BERT}} = \\dfrac{1}{k} \\|A_{k_i}^{l_{BERT}} \\cap A_{k_i}^{l_{GPT}} \\|$$\n",
        "\n",
        "where $\\|A_i^{l_{BERT}} \\cap A_i^{l_{GPT}} \\|$ denotes the number of common elements between the two adjacency lists.\n",
        "\n",
        "The neighborhood overlap between two representations is:\n",
        "$$\\chi_{k}^{l_{GPT}, l_{BERT}} = \\dfrac{1}{N} \\sum_i \\chi_{k_i}^{l_{GPT}, l_{BERT}}$$\n",
        "\n",
        "In the following we will measure the overlap between pairs layers **at the same depth** of iGPT and iBERT.\n",
        "\n",
        "To do so, we use as input the indices of the first 15 neighbors of each data point.\n",
        "\n",
        "You can load the nearest neighbor indices for,  e.g., layer 1 as follows:\n",
        "\n",
        "```\n",
        "indices_igpt = np.load(\"igpt/post_mlp/layer1_post_mlp_indices.npy\")\n",
        "indices_ibert = np.load(\"ibert/post_mlp/layer1_post_mlp_indices.npy\")\n",
        "```\n",
        "the same holds for the other layers.\n",
        "\n",
        "The structure of **indices_igpt** is as follows:\n",
        "\n",
        "```\n",
        "print(indices_igpt)\n",
        "[[0, 8398 ...]\n",
        "[1, 6289, ...]\n",
        "[2, 12688, ..]\n",
        "...\n",
        "[19998, 1231, ... ]\n",
        "[19999, 1266, ... ]]\n",
        "```\n",
        "thus, each datapoint is considered the first nearest neighbor of itself.\n",
        "\n",
        "**Important note:**\n",
        "\n",
        "In order to compute correctly the overlap between representations you must **remove the first element of each row**.\n"
      ],
      "metadata": {
        "id": "f6v_5lbnt2uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement the function that computes overlap between representations here:**\n"
      ],
      "metadata": {
        "id": "WS4EFvPkr2we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_data_overlaps(indices1, indices2, k=15):\n",
        "#     \"\"\"Computes the neighborhood overlap between two representations.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     indices1 : 2D array of ints\n",
        "#         nearest neighbor index matrix of the first representation\n",
        "#     indices2 : 2D array of ints\n",
        "#         nearest neighbor index matrix of the second representation\n",
        "#     k : int\n",
        "#         number of nearest neighbors used to compute the overlap\n",
        "\n",
        "#     Returns\n",
        "#     -------\n",
        "#     overlap : float\n",
        "#         neighborhood overlap between the two representations\n",
        "#     \"\"\"\n",
        "\n",
        "#     assert indices1.shape[0] == indices2.shape[0]\n",
        "#     ndata = indices1.shape[0]\n",
        "#     overlaps = np.empty(ndata)\n",
        "\n",
        "#     for i in range(ndata):\n",
        "#         overlaps[i] = ...\n",
        "#     return np.mean(overlaps)"
      ],
      "metadata": {
        "id": "F_m7Q-MLr9bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute the data overlap between iBERT end iGPT for the layers provided below**:"
      ],
      "metadata": {
        "id": "nN9rgF2LsMD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# layers = np.arange(1, 50, 2)\n",
        "# overlap_igpt_ibert = []\n",
        "# for lay in layers:\n",
        "#     print(lay)\n",
        "#     indices_igpt = np.load(f\"igpt_data/igpt/post_mlp/layer{lay}_post_mlp_indices.npy\")\n",
        "#     indices_ibert = np.load(f\"igpt_data/ibert/post_mlp/layer{lay}_post_mlp_indices.npy\")\n",
        "#     ..."
      ],
      "metadata": {
        "id": "3US3-NmAaJCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the similarity between representations measured by the overlap**"
      ],
      "metadata": {
        "id": "YNeOkKVPsshF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XdU6LGsEssUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantics of iBERT and iGPT representations**\n",
        "The neighborhood overlap can be used as a kNN-like measure of how explicit a given representation is in terms of an external partition of the data. For the ImageNet dataset the external partition is the partition given by the class labels.\n",
        "\n",
        "The class labels can be loaded with:\n",
        "\n",
        "```\n",
        "class_labels = np.load(\"igpt_data/labels.npy\")\n",
        "print(class_labels)\n",
        "np.array([1, 1, 1, ..., 305, 305, 305])\n",
        "```\n",
        "You are give the same subset of the ImageNet training set you used to compute the similarity between iBERT and iGPT representations. This subset contains 20 thousands data point grouped in 100 classes with 200 images each.\n",
        "\n",
        "The overlap computation follows the same guidelines given above with the following modifications:\n",
        "1. the elements of the adjacency lists are the class labels of the neighbors.\n",
        "2. you must count the number of neighbors with the same class labels of the central point and put this number in place of $\\|A_i^{l_{BERT}} \\cap A_i^{l_{GPT}} \\|$\n"
      ],
      "metadata": {
        "id": "55obrzzs6CUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement the function that computes overlap with the ground truth labels for the iBERT representations here**"
      ],
      "metadata": {
        "id": "P68_fb-w98O1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_label_overlap(indices, labels, k=15):\n",
        "#     \"\"\"Computes the neighborhood overlap with ground truth labels.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     indices1 : 2D array (int)\n",
        "#         nearest neighbor index matrix of the first representation\n",
        "#     labels : 1D array (int)\n",
        "#         labels of the datapoints\n",
        "#     k : int\n",
        "#         number of nearest neighbors used to compute the overlap\n",
        "\n",
        "#     Returns\n",
        "#     -------\n",
        "#     overlap : float\n",
        "#         neighborhood overlap with ground truth labels\n",
        "#     \"\"\"\n",
        "#     neighbor_index = indices[:, 1 : k + 1]\n",
        "#     ...\n",
        "#     overlaps = np.mean(overlaps)\n",
        "#     return overlaps"
      ],
      "metadata": {
        "id": "tV70Yhp3-D79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute the data overlap with the gorund truth labels for the iBERT representations here**"
      ],
      "metadata": {
        "id": "5NdlQ9VF-KOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# layers = np.arange(1, 50, 2)\n",
        "# labels = np.load(f\"igpt_data/labels.npy\")\n",
        "# k = 10\n",
        "# ov_ibert = []\n",
        "# for lay in layers:\n",
        "#     print(lay)\n",
        "#     indices_ibert = np.load(f\"igpt_data/ibert/post_mlp/layer{lay}_post_mlp_indices.npy\")\n",
        "#     ov_ibert.append(compute_label_overlap(indices_ibert, labels, k))"
      ],
      "metadata": {
        "id": "wnAaTUh_-WDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the overlap with ground truth labels for iBERT**"
      ],
      "metadata": {
        "id": "pZbhEk0x-qqE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APNds7v6rDSL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}